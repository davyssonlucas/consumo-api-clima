{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd300be",
   "metadata": {},
   "source": [
    "> ![logo100px.jpg](data:image/jpeg;base64,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) Desafio Dados2Dados: Consumindo API de tempo e temperatura com Python e SQL\n",
    "\n",
    "https://colab.research.google.com/gist/ricosuhete/6b97ce298620912687fc985892a52144/api_openweathermap.ipynb\n",
    "\n",
    "\n",
    "## Nota Importante\n",
    "\n",
    "Este projeto aborda um desafio específico de obtenção de dados climáticos da API openweathermap, no entanto, é importante observar o seguinte:\n",
    "\n",
    "- Foi instalado o Apache Airflow numa VM WSL usando o docker-compose e criando uma imagem personalizada;\n",
    "\n",
    "- Para solucionar problemas técnicos, foram criadas duas DAGs no Apache Airflow. Uma DAG é executada diariamente para coletar dados climáticos atuais, enquanto a outra é executada a cada 5 dias para coletar previsões do tempo para os próximos cinco dias.\n",
    "\n",
    "- Ambas as DAGs coletam dados para as cidades de Curitiba, Pinhais, Colombo e Londrina, localizadas no estado do Paraná, Brasil.\n",
    "\n",
    "- Os dados coletados são armazenados em formato Delta Lake, em um bucket chamado \"desafiotempo\", com diferentes caminhos: \"transient/daily_historic\" para dados atuais e \"transient/forecast_historic\" para previsões futuras.\n",
    "\n",
    "É importante destacar que, no momento, o histórico de dados disponíveis não abrange um período de 30 dias completos. Portanto, os resultados obtidos com a execução do código podem não atender às expectativas em relação a um intervalo de 30 dias. No entanto, à medida que mais dados históricos são coletados, será possível executar o código com um período de 30 dias e obter resultados mais precisos.\n",
    "\n",
    "> #### RESOLUÇÃO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72df8dab",
   "metadata": {},
   "source": [
    "Importando bibliotecas e definido váriveis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python_dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python_dotenv\n",
      "Successfully installed python_dotenv-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d5bdaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importando as bibliotecas necessárias\n",
      "Configurando a variável de ambiente PYSPARK_SUBMIT_ARGS\n",
      "Inicando sessão spark\n",
      "Configurando as configurações do MinIO\n"
     ]
    }
   ],
   "source": [
    "# Importa as bibliotecas necessárias\n",
    "print(\"Importando as bibliotecas necessárias\")\n",
    "import os\n",
    "from pyspark.sql import SparkSession, Row\n",
    "#from pyspark.sql.types import StructType, StructField, FloatType, IntegerType, StringType, DateType, LongType, DoubleType, MapType\n",
    "from pyspark.sql.functions import col, explode, from_unixtime, max, min, avg, round, to_date\n",
    "import pyspark.sql.functions as F\n",
    "import requests\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv(\"/home/jovyan/work/desafiotempo/.env\") #insira o caminho do arquivo .env caso use o kernel remoto, caso contrário deixe sem parâmetro\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "\n",
    "# Configura a variável de ambiente PYSPARK_SUBMIT_ARGS\n",
    "print(\"Configurando a variável de ambiente PYSPARK_SUBMIT_ARGS\")\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:3.3.1,io.delta:delta-spark_2.12:3.0.0 --conf \"spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension\" --conf \"spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog\" pyspark-shell'\n",
    "# Inicia uma sessão Spark\n",
    "print(\"Inicando sessão spark\")\n",
    "spark = SparkSession.builder.appName(\"desafio\").getOrCreate()\n",
    "# Configura as configurações do MinIO (caso não estejam definidas em PYSPARK_SUBMIT_ARGS)\n",
    "print(\"Configurando as configurações do MinIO\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"qiBoC6oxvaAdlFCR9I9p\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"vjHjYMCp6fbWyHjO3iFHewSggT0lWiCBFNGVKiu2\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9001\")\n",
    "spark.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889e8772",
   "metadata": {},
   "source": [
    "Função para mapear as informações da API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b33ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_forecast(city):    \n",
    "    url_tempo = f\"https://api.openweathermap.org/data/2.5/forecast?q={city},PR,BR&appid={API_KEY}&units=metric&lang=pt_br\"\n",
    "    response = requests.get(url_tempo)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        # Cria um RDD a partir do JSON\n",
    "        rdd = spark.sparkContext.parallelize([data], numSlices=1)\n",
    "        # Passa o RDD para o método spark.read.json()\n",
    "        df = spark.read.json(rdd, multiLine=True)\n",
    "        df_flat = df.withColumn(\"list_explode\", explode('list'))\\\n",
    "            .withColumn(\"dt_txt\", col('list_explode.dt_txt'))\\\n",
    "            .withColumn(\"dt\", col('list_explode.dt'))\\\n",
    "            .withColumn(\"feels_like\", col('list_explode.main.feels_like'))\\\n",
    "            .withColumn(\"grnd_level\", col('list_explode.main.grnd_level'))\\\n",
    "            .withColumn(\"humidity\", col('list_explode.main.humidity'))\\\n",
    "            .withColumn(\"pressure\", col('list_explode.main.pressure'))\\\n",
    "            .withColumn(\"sea_level\", col('list_explode.main.sea_level'))\\\n",
    "            .withColumn(\"temp\", col('list_explode.main.temp'))\\\n",
    "            .withColumn(\"temp_kf\", col('list_explode.main.temp_kf'))\\\n",
    "            .withColumn(\"temp_max\", col('list_explode.main.temp_max'))\\\n",
    "            .withColumn(\"temp_min\", col('list_explode.main.temp_min'))\\\n",
    "            .withColumn(\"pop\", col('list_explode.pop'))\\\n",
    "            .withColumn(\"rain\", col('list_explode.rain'))\\\n",
    "            .withColumn(\"sys_pod\", col('list_explode.sys.pod'))\\\n",
    "            .withColumn(\"visibility\", col('list_explode.visibility'))\\\n",
    "            .withColumn(\"clouds\", col('list_explode.clouds.all'))\\\n",
    "            .withColumn(\"deg\", col('list_explode.wind.deg'))\\\n",
    "            .withColumn(\"gust\", col('list_explode.wind.gust'))\\\n",
    "            .withColumn(\"speed\", col('list_explode.wind.speed'))\\\n",
    "            .withColumn(\"weather_explode\", explode('list_explode.weather'))\\\n",
    "            .withColumn(\"description\", col('weather_explode.description'))\\\n",
    "            .withColumn(\"icon\", col('weather_explode.icon'))\\\n",
    "            .withColumn(\"id\", col('weather_explode.id'))\\\n",
    "            .withColumn(\"main\", col('weather_explode.main'))\\\n",
    "            .drop(\"weather_explode\")\\\n",
    "            .drop(\"list_explode\")\\\n",
    "            .withColumn(\"coord\", col(\"city.coord\"))\\\n",
    "            .withColumn(\"country\", col(\"city.country\"))\\\n",
    "            .withColumn(\"city_id\", col(\"city.id\"))\\\n",
    "            .withColumn(\"name\", col(\"city.name\"))\\\n",
    "            .withColumn(\"population\", col(\"city.population\"))\\\n",
    "            .withColumn(\"sunrise\", col(\"city.sunrise\"))\\\n",
    "            .withColumn(\"sunset\", col(\"city.sunset\"))\\\n",
    "            .withColumn(\"timezone\", col(\"city.timezone\"))\\\n",
    "            .drop(\"city\")\\\n",
    "            .drop(\"list\")        \n",
    "        return df_flat\n",
    "\n",
    "    else:\n",
    "        print(f\"INFO: {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aab14b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request_json(city):\n",
    "    url_tempo = f\"https://api.openweathermap.org/data/2.5/weather?q={city},PR,BR&appid={API_KEY}&units=metric&lang=pt_br\"\n",
    "    response = requests.get(url_tempo)\n",
    "    if response.status_code == 200:\n",
    "        data1 = response.json()\n",
    "        rdd = spark.sparkContext.parallelize([data1], numSlices=1)\n",
    "        # Passa o RDD para o método spark.read.json()\n",
    "        df_city_temp = spark.read.json(rdd, multiLine=True)\n",
    "        #df_city_temp.printSchema()\n",
    "\n",
    "        df_flat = df_city_temp.withColumn(\"name\", col('name'))\\\n",
    "                                    .withColumn(\"temp\", col('main.temp'))\\\n",
    "                                    .withColumn(\"temp_max\", col('main.temp_max'))\\\n",
    "                                    .withColumn(\"temp_min\", col('main.temp_min'))\\\n",
    "                                    .withColumn(\"clouds\", col('clouds.all'))\\\n",
    "                                    .withColumn(\"cod\", col('cod'))\\\n",
    "                                    .withColumn(\"dt\", from_unixtime(col('dt')))\\\n",
    "                                    .withColumn(\"timezone\", col('timezone'))\\\n",
    "                                    .withColumn(\"main_feels_like\", col('main.feels_like'))\\\n",
    "                                    .withColumn(\"humidity\", col('main.humidity'))\\\n",
    "                                    .withColumn(\"pressure\", col('main.pressure'))\\\n",
    "                                    .withColumn(\"country\", col('sys.country'))\\\n",
    "                                    .withColumn(\"sys_id\", col('sys.id'))\\\n",
    "                                    .withColumn(\"sunrise\", col('sys.sunrise'))\\\n",
    "                                    .withColumn(\"sunset\", col('sys.sunset'))\\\n",
    "                                    .withColumn(\"deg\", col('wind.deg'))\\\n",
    "                                    .withColumn(\"speed\", col('wind.speed'))\\\n",
    "                                    .withColumn(\"weather_explode\", explode('weather'))\\\n",
    "                                    .withColumn(\"id\", col('id'))\\\n",
    "                                    .withColumn(\"main\", col('weather_explode.main'))\\\n",
    "                                    .withColumn(\"icon\", col('weather_explode.icon'))\\\n",
    "                                    .withColumn(\"weather_id\", col('weather_explode.id'))\\\n",
    "                                    .drop(\"weather_explode\")\\\n",
    "                                    .drop(\"sys\")\\\n",
    "                                    .drop(\"weather\")\\\n",
    "                                    .drop(\"wind\")\\\n",
    "                                    .drop(\"base\")\n",
    "        return df_flat\n",
    "    else:\n",
    "        print(f\"INFO:{response.text}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7cf75",
   "metadata": {},
   "source": [
    "#### - Obter a temperatura atual para uma lista de cidades do seu estado e armazenar os resultados em uma tabela SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac25a936-a681-499f-bfe9-cb10a7d7efff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+-----+--------+--------+------+\n",
      "|    name|                 dt| temp|temp_max|temp_min|  main|\n",
      "+--------+-------------------+-----+--------+--------+------+\n",
      "|Curitiba|2023-11-08 04:46:04|14.27|   14.47|   13.93|Clouds|\n",
      "+--------+-------------------+-----+--------+--------+------+\n",
      "\n",
      "+-------+-------------------+-----+--------+--------+------+\n",
      "|   name|                 dt| temp|temp_max|temp_min|  main|\n",
      "+-------+-------------------+-----+--------+--------+------+\n",
      "|Pinhais|2023-11-08 04:46:08|14.32|   14.64|    14.1|Clouds|\n",
      "+-------+-------------------+-----+--------+--------+------+\n",
      "\n",
      "+-------+-------------------+-----+--------+--------+------+\n",
      "|   name|                 dt| temp|temp_max|temp_min|  main|\n",
      "+-------+-------------------+-----+--------+--------+------+\n",
      "|Colombo|2023-11-08 04:42:29|13.43|    13.9|   13.36|Clouds|\n",
      "+-------+-------------------+-----+--------+--------+------+\n",
      "\n",
      "+--------+-------------------+-----+--------+--------+-----+\n",
      "|    name|                 dt| temp|temp_max|temp_min| main|\n",
      "+--------+-------------------+-----+--------+--------+-----+\n",
      "|Londrina|2023-11-08 04:42:50|22.81|   22.81|   22.81|Clear|\n",
      "+--------+-------------------+-----+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_city_name = [\"curitiba\", \"pinhais\", \"colombo\", \"londrina\"]\n",
    "for city in list_city_name:    \n",
    "    df_city_temp = request_json(city)\n",
    "    df_city_temp = df_city_temp.select(\"name\", \"dt\", \"temp\", \"temp_max\", \"temp_min\", \"main\")\n",
    "    df_city_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55804d9",
   "metadata": {},
   "source": [
    "#### - Analisar a temperatura máxima e mínima para cada cidade em um período de 30 dias e exibir os resultados em uma tabela SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a46a28e-a391-4710-8f7a-c519532a2e3f",
   "metadata": {},
   "source": [
    "> - Para atender este item, estou acessando os dados que estão sendo salvos para compor o historico de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37c78001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------+------------------+\n",
      "|  Cidade|media_temp_max| Cidade|    media_temp_min|\n",
      "+--------+--------------+-------+------------------+\n",
      "|Londrina|      20.47325|Colombo|16.086750000000002|\n",
      "+--------+--------------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Especifique o caminho dos dados Delta\n",
    "minio_bucket_name = \"desafiotempo\"\n",
    "minio_path = \"transient/forecast_historic\"\n",
    "format = \"delta\"\n",
    "\n",
    "table_path = f\"s3a://{minio_bucket_name}/{minio_path}\"\n",
    "#lendo o delta\n",
    "df_temp = spark.read.format(format).load(table_path)\n",
    "df_temp = df_temp.select(\"name\", \"temp_max\", \"temp_min\", \"dt\")\n",
    "\n",
    "\n",
    "df_temp = df_temp.withColumn(\"date\", from_unixtime(col(\"dt\")))\n",
    "\n",
    "# Defina a data de início e fim do intervalo de 30 dias\n",
    "data_inicio = \"2023-10-31\"\n",
    "data_fim = \"2023-11-30\"\n",
    "\n",
    "# Selecione o intervalo de 30 dias\n",
    "df_temp = df_temp.filter((col(\"date\") >= data_inicio) & (col(\"date\") <= data_fim))\n",
    "\n",
    "# Calcular a média da coluna \"temp_max\"\n",
    "average_temp_max = df_temp.groupBy(\"name\").agg(avg(\"temp_max\").alias(\"media_temp_max\"))\n",
    "average_temp_max = average_temp_max.withColumnRenamed(\"name\", \"Cidade\")\n",
    "# Calcular a média da coluna \"temp_min\"\n",
    "average_temp_min = df_temp.groupBy(\"name\").agg(avg(\"temp_min\").alias(\"media_temp_min\"))\n",
    "average_temp_min = average_temp_min.withColumnRenamed(\"name\", \"Cidade\")\n",
    "\n",
    "# Encontrar a cidade com a maior média de temperatura máxima\n",
    "city_with_max_temp_max = average_temp_max.orderBy(col(\"media_temp_max\").desc()).first()\n",
    "\n",
    "# Encontrar a cidade com a menor média de temperatura mínima\n",
    "city_with_min_temp_min = average_temp_min.orderBy(col(\"media_temp_min\").asc()).first()\n",
    "\n",
    "# Criar DataFrames para as cidades com a maior e a menor média de temperatura:\n",
    "city_with_max_temp_max = spark.createDataFrame([Row(**city_with_max_temp_max.asDict())])\n",
    "city_with_min_temp_min = spark.createDataFrame([Row(**city_with_min_temp_min.asDict())])\n",
    "\n",
    "\n",
    "city_with_max_temp_max.createOrReplaceTempView(\"city_with_max_temp_max\")\n",
    "city_with_min_temp_min.createOrReplaceTempView(\"city_with_min_temp_min\")\n",
    "spark.sql(\"SELECT * FROM city_with_max_temp_max, city_with_min_temp_min\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b2c41",
   "metadata": {},
   "source": [
    "#### - Determinar a cidade com a maior diferença entre a temperatura máxima e mínima e exibir o resultado em uma tabela SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ad14f-b635-4ea8-9217-4e46fc7ee0f8",
   "metadata": {},
   "source": [
    "> - Para atender este item, estou acessando os dados que estão sendo salvos para compor o historico de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "323fd814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+-------------------+------------------+\n",
      "|    name|temp_max|temp_min|             dt_txt|    diferença_temp|\n",
      "+--------+--------+--------+-------------------+------------------+\n",
      "|Curitiba|   21.27|   19.25|2023-10-31 12:00:00|2.0199999999999996|\n",
      "+--------+--------+--------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Especifique o caminho dos dados Delta\n",
    "minio_bucket_name = \"desafiotempo\"\n",
    "minio_path = \"transient/forecast_historic\"\n",
    "format = \"delta\"\n",
    "\n",
    "table_path = f\"s3a://{minio_bucket_name}/{minio_path}\"\n",
    "#lendo o delta\n",
    "df_temp = spark.read.format(format).load(table_path)\n",
    "df_temp = df_temp.select(\"name\", \"temp_max\", \"temp_min\", \"dt_txt\")\n",
    "# Calcular a diferença entre a temperatura máxima e mínima\n",
    "df_temp = df_temp.withColumn(\"diferença_temp\", col(\"temp_max\") - col(\"temp_min\"))\n",
    "\n",
    "# Encontrar a cidade com a maior diferença\n",
    "city_with_max_temp_diff = df_temp.orderBy(col(\"diferença_temp\").desc()).first()\n",
    "\n",
    "# Criar um DataFrame para a cidade com a maior diferença\n",
    "max_temp_diff_df = spark.createDataFrame([Row(**city_with_max_temp_diff.asDict())])\n",
    "\n",
    "# Exibir a cidade com a maior diferença em uma tabela SQL\n",
    "max_temp_diff_df.createOrReplaceTempView(\"max_temp_diff_city\")\n",
    "spark.sql(\"SELECT * FROM max_temp_diff_city\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1da9b8e",
   "metadata": {},
   "source": [
    "#### - Identificar a cidade mais quente e a cidade mais fria em um período de 30 dias e exibir os resultados em uma tabela SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbecd62b",
   "metadata": {},
   "source": [
    "> - Para atender este item, estou acessando os dados que estão sendo salvos para compor o historico de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "734c6eae-a166-4ad9-8c63-7a6213adf31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cidades com a maior e a menor média de temperatura:\n",
      "+--------+-----+-------+----+\n",
      "|  Cidade| temp| Cidade|temp|\n",
      "+--------+-----+-------+----+\n",
      "|Londrina|20.47|Colombo|16.1|\n",
      "+--------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Especifique o caminho dos dados Delta\n",
    "minio_bucket_name = \"desafiotempo\"\n",
    "minio_path = \"transient/forecast_historic\"\n",
    "format = \"delta\"\n",
    "\n",
    "table_path = f\"s3a://{minio_bucket_name}/{minio_path}\"\n",
    "#lendo o delta\n",
    "df_temp2 = spark.read.format(format).load(table_path)\n",
    "df_temp2 = df_temp2.select(\"name\", \"temp\", \"dt\")\n",
    "\n",
    "df_temp2 = df_temp2.withColumn(\"date\", from_unixtime(col(\"dt\")))\n",
    "\n",
    "# Defina a data de início e fim do intervalo de 30 dias\n",
    "data_inicio = \"2023-10-31\"\n",
    "data_fim = \"2023-11-30\"\n",
    "\n",
    "# Seleciona o intervalo de 30 dias\n",
    "df_temp2 = df_temp2.filter((col(\"date\") >= data_inicio) & (col(\"date\") <= data_fim))\n",
    "\n",
    "# Calcular a média da coluna \"temp\"\n",
    "average_temp = df_temp2.groupBy(\"name\").agg(avg(\"temp\").alias(\"temp\"))\n",
    "average_temp = average_temp.withColumnRenamed(\"name\", \"Cidade\")\n",
    "\n",
    "# Arredondar a coluna \"temp\" para duas casas decimais\n",
    "average_temp = average_temp.withColumn(\"temp\", round(average_temp[\"temp\"], 2))\n",
    "\n",
    "# Encontrar a cidade com a maior média de temperatura\n",
    "city_with_max_temp = average_temp.orderBy(col(\"temp\").desc()).first()\n",
    "city_with_max_temp = spark.createDataFrame([Row(**city_with_max_temp.asDict())])\n",
    "city_with_max_temp.createOrReplaceTempView(\"city_with_max_temp\")\n",
    "\n",
    "# Encontrar a cidade com a menor média de temperatura\n",
    "city_with_min_temp = average_temp.orderBy(col(\"temp\").asc()).first()\n",
    "city_with_min_temp = spark.createDataFrame([Row(**city_with_min_temp.asDict())])\n",
    "city_with_min_temp.createOrReplaceTempView(\"city_with_min_temp\")\n",
    "\n",
    "# Criar tabela SQL para as cidades com a maior e a menor média de temperatura:\n",
    "print(\"Cidades com a maior e a menor média de temperatura:\")\n",
    "spark.sql(\"SELECT * FROM city_with_max_temp, city_with_min_temp\").show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b649b9e",
   "metadata": {},
   "source": [
    "#### - Calcular a média da temperatura para cada dia em um período de 30 dias e exibir os resultados em uma tabela SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496c786",
   "metadata": {},
   "source": [
    "> - Para atender este item, estou acessando os dados que estão sendo salvos para compor o historico de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6ce2798f-2a36-4245-b7f2-9b993f75c25c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------+\n",
      "|    name|      date|daily_avg_temp|\n",
      "+--------+----------+--------------+\n",
      "| Colombo|2023-10-31|         18.14|\n",
      "| Colombo|2023-11-01|          16.0|\n",
      "| Colombo|2023-11-02|         15.86|\n",
      "| Colombo|2023-11-03|         17.63|\n",
      "| Colombo|2023-11-04|         15.38|\n",
      "| Colombo|2023-11-05|         11.45|\n",
      "|Curitiba|2023-10-31|         18.99|\n",
      "|Curitiba|2023-11-01|         16.55|\n",
      "|Curitiba|2023-11-02|         16.36|\n",
      "|Curitiba|2023-11-03|         17.83|\n",
      "|Curitiba|2023-11-04|         15.62|\n",
      "|Curitiba|2023-11-05|         12.34|\n",
      "|Londrina|2023-10-31|         20.36|\n",
      "|Londrina|2023-11-01|         20.52|\n",
      "|Londrina|2023-11-02|          21.4|\n",
      "|Londrina|2023-11-03|         22.98|\n",
      "|Londrina|2023-11-04|         19.05|\n",
      "|Londrina|2023-11-05|          15.2|\n",
      "| Pinhais|2023-10-31|         19.11|\n",
      "| Pinhais|2023-11-01|         16.51|\n",
      "| Pinhais|2023-11-02|         16.33|\n",
      "| Pinhais|2023-11-03|          18.0|\n",
      "| Pinhais|2023-11-04|         15.93|\n",
      "| Pinhais|2023-11-05|         12.38|\n",
      "+--------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Especifique o caminho dos dados Delta\n",
    "minio_bucket_name = \"desafiotempo\"\n",
    "minio_path = \"transient/forecast_historic\"\n",
    "format = \"delta\"\n",
    "\n",
    "table_path = f\"s3a://{minio_bucket_name}/{minio_path}\"\n",
    "\n",
    "# Carregue os dados Delta Lake\n",
    "df_temp3 = spark.read.format(format).load(table_path)\n",
    "\n",
    "# Converter a coluna \"dt_txt\" para o nível do dia\n",
    "df_temp3 = df_temp3.withColumn(\"date\", to_date(df_temp3[\"dt_txt\"]))\n",
    "\n",
    "# Calcule a média da temperatura diária\n",
    "average_temp3 = df_temp3.groupBy(\"name\", \"date\").agg(avg(\"temp\").alias(\"daily_avg_temp\"))\n",
    "\n",
    "# Arredonde a coluna \"daily_avg_temp\" para duas casas decimais\n",
    "average_temp3 = average_temp3.withColumn(\"daily_avg_temp\", round(average_temp3[\"daily_avg_temp\"], 2))\n",
    "\n",
    "# Crie uma tabela temporária\n",
    "average_temp3.createOrReplaceTempView(\"daily_avg_temp_table\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT name, date, daily_avg_temp FROM daily_avg_temp_table\n",
    "WHERE date BETWEEN '2023-10-31' AND '2023-11-30' ORDER BY name ASC, date ASC\"\"\").show(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47bb79",
   "metadata": {},
   "source": [
    "#### - Identificar as cidades com as maiores e menores variações de temperatura em um período de 30 dias e exibir os resultados em uma tabela SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4051a0",
   "metadata": {},
   "source": [
    "> - Para atender este item, estou acessando os dados que estão sendo salvos para compor o historico de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87c2e5ad-c4ff-461c-b6ee-6ced6731d9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|    name|dif_temp|\n",
      "+--------+--------+\n",
      "|Curitiba|   0.081|\n",
      "| Colombo|   0.056|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicialize a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Variação de Temperatura\").getOrCreate()\n",
    "\n",
    "# Carregue os dados Delta\n",
    "minio_bucket_name = \"desafiotempo\"\n",
    "minio_path = \"transient/forecast_historic\"\n",
    "format = \"delta\"\n",
    "table_path = f\"s3a://{minio_bucket_name}/{minio_path}\"\n",
    "df_temp4 = spark.read.format(format).load(table_path)\n",
    "df_temp4 = df_temp4.select(\"name\", \"temp_max\", \"temp_min\", \"dt\")\n",
    "\n",
    "df_temp4 = df_temp4.withColumn(\"date\", from_unixtime(col(\"dt\")))\n",
    "\n",
    "# Defina a data de início e fim do intervalo de 30 dias\n",
    "data_inicio = \"2023-10-31\"\n",
    "data_fim = \"2023-11-30\"\n",
    "\n",
    "# Selecione o intervalo de 30 dias\n",
    "df_temp4 = df_temp4.filter((col(\"date\") >= data_inicio) & (col(\"date\") <= data_fim))\n",
    "\n",
    "# Agregar as médias de temp_max e temp_min para cada cidade\n",
    "df_temp4 = df_temp4.groupBy(\"name\").agg(avg(col(\"temp_max\")).alias(\"media_temp_max\"), avg(col(\"temp_min\")).alias(\"media_temp_min\"))\n",
    "\n",
    "# Calcular a diferença entre a temperatura máxima e mínima\n",
    "df_temp4 = df_temp4.withColumn(\"diferença_temp\", col(\"media_temp_max\") - col(\"media_temp_min\"))\n",
    "df_temp4 = df_temp4.withColumn(\"dif_temp\", round(col(\"diferença_temp\"), 3))\n",
    "# Encontra a cidade com a maior diferença de temperatura\n",
    "city_with_max_temp_diff = df_temp4.orderBy(col(\"dif_temp\").desc()).first()\n",
    "\n",
    "# Encontra a cidade com a menor diferença de temperatura\n",
    "city_with_min_temp_diff = df_temp4.orderBy(col(\"dif_temp\")).first()\n",
    "\n",
    "# Exibir os resultados em uma tabela SQL\n",
    "results_df = spark.createDataFrame([city_with_max_temp_diff, city_with_min_temp_diff])\n",
    "results_df.createOrReplaceTempView(\"variacoes_temperatura\")\n",
    "spark.sql(\"SELECT name, dif_temp  FROM variacoes_temperatura\").show()\n",
    "\n",
    "# Encerre a sessão Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94cd42",
   "metadata": {},
   "source": [
    "#### - Obter a previsão do tempo para uma lista de cidades do seu estado nos próximos 7 dias e armazenar os resultados em uma tabela SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59625571-f774-4b47-9750-1edf73127c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-------------------+\n",
      "|    name| temp|  main|               date|\n",
      "+--------+-----+------+-------------------+\n",
      "| Colombo|15.58|  Rain|2023-11-01 06:00:00|\n",
      "| Colombo|15.46|  Rain|2023-11-01 09:00:00|\n",
      "| Colombo|16.72|Clouds|2023-11-01 12:00:00|\n",
      "| Colombo|20.33|  Rain|2023-11-01 15:00:00|\n",
      "| Colombo|18.44|Clouds|2023-11-01 18:00:00|\n",
      "| Colombo|15.39|Clouds|2023-11-01 21:00:00|\n",
      "| Colombo|13.84|Clouds|2023-11-02 00:00:00|\n",
      "| Colombo|13.65|Clouds|2023-11-02 03:00:00|\n",
      "| Colombo|13.68|Clouds|2023-11-02 06:00:00|\n",
      "| Colombo|13.84|Clouds|2023-11-02 09:00:00|\n",
      "| Colombo|15.98|  Rain|2023-11-02 12:00:00|\n",
      "| Colombo|21.36|Clouds|2023-11-02 15:00:00|\n",
      "| Colombo|17.04|  Rain|2023-11-02 18:00:00|\n",
      "| Colombo|16.45|  Rain|2023-11-02 21:00:00|\n",
      "| Colombo|15.39|Clouds|2023-11-03 00:00:00|\n",
      "| Colombo| 15.0|  Rain|2023-11-03 03:00:00|\n",
      "| Colombo|14.73|  Rain|2023-11-03 06:00:00|\n",
      "| Colombo|14.64|  Rain|2023-11-03 09:00:00|\n",
      "| Colombo|19.01|  Rain|2023-11-03 12:00:00|\n",
      "| Colombo|24.46|  Rain|2023-11-03 15:00:00|\n",
      "| Colombo|20.15|  Rain|2023-11-03 18:00:00|\n",
      "| Colombo|19.96|  Rain|2023-11-03 21:00:00|\n",
      "| Colombo| 16.1|  Rain|2023-11-04 00:00:00|\n",
      "| Colombo|12.16|Clouds|2023-11-04 03:00:00|\n",
      "| Colombo| 8.79|Clouds|2023-11-04 06:00:00|\n",
      "| Colombo| 7.25| Clear|2023-11-04 09:00:00|\n",
      "| Colombo|14.56| Clear|2023-11-04 12:00:00|\n",
      "| Colombo|20.48| Clear|2023-11-04 15:00:00|\n",
      "| Colombo|22.23| Clear|2023-11-04 18:00:00|\n",
      "| Colombo|17.85| Clear|2023-11-04 21:00:00|\n",
      "| Colombo|13.47| Clear|2023-11-05 00:00:00|\n",
      "| Colombo|11.17| Clear|2023-11-05 03:00:00|\n",
      "| Colombo| 9.56| Clear|2023-11-05 06:00:00|\n",
      "| Colombo|11.61|Clouds|2023-11-05 09:00:00|\n",
      "| Colombo|17.56|Clouds|2023-11-05 12:00:00|\n",
      "| Colombo|23.46| Clear|2023-11-05 15:00:00|\n",
      "| Colombo|24.39| Clear|2023-11-05 18:00:00|\n",
      "| Colombo|16.09|Clouds|2023-11-05 21:00:00|\n",
      "| Colombo|13.23|Clouds|2023-11-06 00:00:00|\n",
      "| Colombo|12.37|Clouds|2023-11-06 03:00:00|\n",
      "|Curitiba|16.07|  Rain|2023-11-01 06:00:00|\n",
      "|Curitiba|16.03|  Rain|2023-11-01 09:00:00|\n",
      "|Curitiba|17.13|Clouds|2023-11-01 12:00:00|\n",
      "|Curitiba|21.48|Clouds|2023-11-01 15:00:00|\n",
      "|Curitiba|19.43|  Rain|2023-11-01 18:00:00|\n",
      "|Curitiba|16.28|Clouds|2023-11-01 21:00:00|\n",
      "|Curitiba| 14.8|Clouds|2023-11-02 00:00:00|\n",
      "|Curitiba|14.27|Clouds|2023-11-02 03:00:00|\n",
      "|Curitiba|14.31|Clouds|2023-11-02 06:00:00|\n",
      "|Curitiba|14.46|Clouds|2023-11-02 09:00:00|\n",
      "|Curitiba|15.95|  Rain|2023-11-02 12:00:00|\n",
      "|Curitiba|20.84|Clouds|2023-11-02 15:00:00|\n",
      "|Curitiba|17.61|  Rain|2023-11-02 18:00:00|\n",
      "|Curitiba|16.78|  Rain|2023-11-02 21:00:00|\n",
      "|Curitiba|15.73|Clouds|2023-11-03 00:00:00|\n",
      "|Curitiba| 15.5|  Rain|2023-11-03 03:00:00|\n",
      "|Curitiba|14.96|  Rain|2023-11-03 06:00:00|\n",
      "|Curitiba|15.34|  Rain|2023-11-03 09:00:00|\n",
      "|Curitiba| 19.5|  Rain|2023-11-03 12:00:00|\n",
      "|Curitiba|24.15|  Rain|2023-11-03 15:00:00|\n",
      "|Curitiba|20.92|  Rain|2023-11-03 18:00:00|\n",
      "|Curitiba|20.29|  Rain|2023-11-03 21:00:00|\n",
      "|Curitiba| 16.0|  Rain|2023-11-04 00:00:00|\n",
      "|Curitiba|12.62|Clouds|2023-11-04 03:00:00|\n",
      "|Curitiba| 9.76|Clouds|2023-11-04 06:00:00|\n",
      "|Curitiba| 8.08| Clear|2023-11-04 09:00:00|\n",
      "|Curitiba| 14.4| Clear|2023-11-04 12:00:00|\n",
      "|Curitiba|20.48| Clear|2023-11-04 15:00:00|\n",
      "|Curitiba|22.76| Clear|2023-11-04 18:00:00|\n",
      "|Curitiba| 19.2| Clear|2023-11-04 21:00:00|\n",
      "|Curitiba|14.73| Clear|2023-11-05 00:00:00|\n",
      "|Curitiba|12.22| Clear|2023-11-05 03:00:00|\n",
      "|Curitiba|10.64| Clear|2023-11-05 06:00:00|\n",
      "|Curitiba|12.61|Clouds|2023-11-05 09:00:00|\n",
      "|Curitiba|17.63|Clouds|2023-11-05 12:00:00|\n",
      "|Curitiba|23.53| Clear|2023-11-05 15:00:00|\n",
      "|Curitiba|25.01| Clear|2023-11-05 18:00:00|\n",
      "|Curitiba|17.66|Clouds|2023-11-05 21:00:00|\n",
      "|Curitiba| 14.1|Clouds|2023-11-06 00:00:00|\n",
      "|Curitiba|13.28|Clouds|2023-11-06 03:00:00|\n",
      "|Londrina|21.81|Clouds|2023-11-01 06:00:00|\n",
      "|Londrina| 20.8|Clouds|2023-11-01 09:00:00|\n",
      "|Londrina|21.93|Clouds|2023-11-01 12:00:00|\n",
      "|Londrina|25.09|Clouds|2023-11-01 15:00:00|\n",
      "|Londrina|22.79|  Rain|2023-11-01 18:00:00|\n",
      "|Londrina|21.89|  Rain|2023-11-01 21:00:00|\n",
      "|Londrina|20.25|  Rain|2023-11-02 00:00:00|\n",
      "|Londrina|19.65|  Rain|2023-11-02 03:00:00|\n",
      "|Londrina|17.69|  Rain|2023-11-02 06:00:00|\n",
      "|Londrina|16.82|  Rain|2023-11-02 09:00:00|\n",
      "|Londrina|20.36|  Rain|2023-11-02 12:00:00|\n",
      "|Londrina|23.38|  Rain|2023-11-02 15:00:00|\n",
      "|Londrina|26.53|Clouds|2023-11-02 18:00:00|\n",
      "|Londrina|24.34|Clouds|2023-11-02 21:00:00|\n",
      "|Londrina|21.29|Clouds|2023-11-03 00:00:00|\n",
      "|Londrina|20.14|Clouds|2023-11-03 03:00:00|\n",
      "|Londrina|19.15|Clouds|2023-11-03 06:00:00|\n",
      "|Londrina|18.66|Clouds|2023-11-03 09:00:00|\n",
      "|Londrina| 21.7|  Rain|2023-11-03 12:00:00|\n",
      "|Londrina|28.85|  Rain|2023-11-03 15:00:00|\n",
      "|Londrina|27.87|Clouds|2023-11-03 18:00:00|\n",
      "|Londrina|24.27| Clear|2023-11-03 21:00:00|\n",
      "|Londrina|19.87| Clear|2023-11-04 00:00:00|\n",
      "|Londrina|16.97| Clear|2023-11-04 03:00:00|\n",
      "|Londrina|13.73| Clear|2023-11-04 06:00:00|\n",
      "|Londrina|12.05| Clear|2023-11-04 09:00:00|\n",
      "|Londrina|17.97| Clear|2023-11-04 12:00:00|\n",
      "|Londrina|23.56| Clear|2023-11-04 15:00:00|\n",
      "|Londrina|25.25| Clear|2023-11-04 18:00:00|\n",
      "|Londrina|21.07| Clear|2023-11-04 21:00:00|\n",
      "|Londrina|16.31| Clear|2023-11-05 00:00:00|\n",
      "|Londrina|15.15| Clear|2023-11-05 03:00:00|\n",
      "|Londrina|14.24| Clear|2023-11-05 06:00:00|\n",
      "|Londrina|13.78| Clear|2023-11-05 09:00:00|\n",
      "|Londrina| 21.3| Clear|2023-11-05 12:00:00|\n",
      "|Londrina|27.02| Clear|2023-11-05 15:00:00|\n",
      "|Londrina|29.09| Clear|2023-11-05 18:00:00|\n",
      "|Londrina|24.44| Clear|2023-11-05 21:00:00|\n",
      "|Londrina|19.94|Clouds|2023-11-06 00:00:00|\n",
      "|Londrina|17.91|Clouds|2023-11-06 03:00:00|\n",
      "| Pinhais|16.19|  Rain|2023-11-01 06:00:00|\n",
      "| Pinhais|16.12|  Rain|2023-11-01 09:00:00|\n",
      "| Pinhais|17.11|Clouds|2023-11-01 12:00:00|\n",
      "| Pinhais|20.77|  Rain|2023-11-01 15:00:00|\n",
      "| Pinhais|18.75|  Rain|2023-11-01 18:00:00|\n",
      "| Pinhais|15.92|Clouds|2023-11-01 21:00:00|\n",
      "| Pinhais|14.59|Clouds|2023-11-02 00:00:00|\n",
      "| Pinhais|14.27|Clouds|2023-11-02 03:00:00|\n",
      "| Pinhais|14.34|Clouds|2023-11-02 06:00:00|\n",
      "| Pinhais|14.52|Clouds|2023-11-02 09:00:00|\n",
      "| Pinhais| 16.1|  Rain|2023-11-02 12:00:00|\n",
      "| Pinhais|20.76|Clouds|2023-11-02 15:00:00|\n",
      "| Pinhais|17.54|  Rain|2023-11-02 18:00:00|\n",
      "| Pinhais|16.75|  Rain|2023-11-02 21:00:00|\n",
      "| Pinhais|15.83|Clouds|2023-11-03 00:00:00|\n",
      "| Pinhais|15.52|  Rain|2023-11-03 03:00:00|\n",
      "| Pinhais|14.92|  Rain|2023-11-03 06:00:00|\n",
      "| Pinhais|15.24|  Rain|2023-11-03 09:00:00|\n",
      "| Pinhais|19.51|  Rain|2023-11-03 12:00:00|\n",
      "| Pinhais|24.83|  Rain|2023-11-03 15:00:00|\n",
      "| Pinhais|21.01|  Rain|2023-11-03 18:00:00|\n",
      "| Pinhais|20.55|  Rain|2023-11-03 21:00:00|\n",
      "| Pinhais|16.43|  Rain|2023-11-04 00:00:00|\n",
      "| Pinhais|12.89|Clouds|2023-11-04 03:00:00|\n",
      "| Pinhais|  9.9|Clouds|2023-11-04 06:00:00|\n",
      "| Pinhais| 8.27| Clear|2023-11-04 09:00:00|\n",
      "| Pinhais|14.89| Clear|2023-11-04 12:00:00|\n",
      "| Pinhais|20.93| Clear|2023-11-04 15:00:00|\n",
      "| Pinhais|23.01| Clear|2023-11-04 18:00:00|\n",
      "| Pinhais|19.01| Clear|2023-11-04 21:00:00|\n",
      "| Pinhais|14.61| Clear|2023-11-05 00:00:00|\n",
      "| Pinhais|12.24| Clear|2023-11-05 03:00:00|\n",
      "| Pinhais|11.19| Clear|2023-11-05 06:00:00|\n",
      "| Pinhais|12.79|Clouds|2023-11-05 09:00:00|\n",
      "| Pinhais|17.64|Clouds|2023-11-05 12:00:00|\n",
      "| Pinhais|23.18| Clear|2023-11-05 15:00:00|\n",
      "| Pinhais|24.11| Clear|2023-11-05 18:00:00|\n",
      "| Pinhais|16.84|Clouds|2023-11-05 21:00:00|\n",
      "| Pinhais|13.99|Clouds|2023-11-06 00:00:00|\n",
      "| Pinhais|13.24|Clouds|2023-11-06 03:00:00|\n",
      "+--------+-----+------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicialize a sessão Spark\n",
    "spark = SparkSession.builder.appName(\"Exemplo\").getOrCreate()\n",
    "\n",
    "list_city_name = [\"curitiba\", \"pinhais\", \"colombo\", \"londrina\"]\n",
    "\n",
    "# Crie uma lista de DataFrames para cada cidade\n",
    "dfs = []\n",
    "for city in list_city_name:\n",
    "    df = api_forecast(city)\n",
    "    df = df.withColumn(\"date\", from_unixtime(col(\"dt\")))\n",
    "\n",
    "    # Defina a data de início e fim do intervalo de 30 dias\n",
    "    now = datetime.now()\n",
    "    day = timedelta(days=1)\n",
    "    data_inicio = now\n",
    "    data_fim = now + 7 * day\n",
    "\n",
    "    # Selecione o intervalo de 30 dias\n",
    "    df = df.filter((col(\"date\") >= data_inicio) & (col(\"date\") <= data_fim))\n",
    "    df.createOrReplaceTempView(f\"forecast{city}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "# Combine os DataFrames de todas as cidades usando union\n",
    "combined_df = dfs[0]\n",
    "for df in dfs[1:]:\n",
    "    combined_df = combined_df.union(df)\n",
    "\n",
    "\n",
    "combined_df= combined_df.select(\"name\", \"temp\", \"main\", \"date\")\n",
    "# Exiba os resultados\n",
    "combined_df.orderBy(\"name\", \"date\").show(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a730a",
   "metadata": {},
   "source": [
    "#### - Identificar a cidade com a maior quantidade de dias chuvosos em um período de 30 dias e exibir o resultado em uma tabela SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082ec3d8",
   "metadata": {},
   "source": [
    "> - Para atender este item, estou acessando os dados que estão sendo salvos para compor o historico de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d79b1d5-6992-4bf0-ba1f-47a4262f6efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank das cidades com a maior quantidade de dias chuvosos:\n",
      "+--------+-----+\n",
      "|    name|count|\n",
      "+--------+-----+\n",
      "|Curitiba|   21|\n",
      "|Londrina|   15|\n",
      "| Pinhais|   21|\n",
      "| Colombo|   22|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Especifique o caminho dos dados Delta\n",
    "minio_bucket_name = \"desafiotempo\"\n",
    "minio_path = \"transient/forecast_historic\"\n",
    "format = \"delta\"\n",
    "\n",
    "table_path = f\"s3a://{minio_bucket_name}/{minio_path}\"\n",
    "\n",
    "# Carregue os dados Delta Lake\n",
    "df_rain = spark.read.format(format).load(table_path)\n",
    "df_rain = df_rain.select(\"name\", \"main\", \"dt\")\n",
    "\n",
    "df_rain = df_rain.withColumn(\"date\", from_unixtime(col(\"dt\")))\n",
    "\n",
    "# Defina a data de início e fim do intervalo de 30 dias\n",
    "data_inicio = \"2023-10-31\"\n",
    "data_fim = \"2023-11-30\"\n",
    "\n",
    "# Selecione o intervalo de 30 dias\n",
    "df_rain = df_rain.filter((col(\"date\") >= data_inicio) & (col(\"date\") <= data_fim))\n",
    "\n",
    "df_rain = df_rain.filter(col(\"main\") == \"Rain\") \\\n",
    "    .groupBy(\"name\") \\\n",
    "    .count()\n",
    "\n",
    "df_rain = df_rain.createOrReplaceTempView(\"avg_days_rain\")\n",
    "\n",
    "print(\"Rank das cidades com a maior quantidade de dias chuvosos:\")\n",
    "spark.sql(\"SELECT * FROM avg_days_rain\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dc2ce1",
   "metadata": {},
   "source": [
    "#### - Calcular a média de umidade para cada dia em um período de 30 dias e exibir os resultados em uma tabela SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9370d0af",
   "metadata": {},
   "source": [
    "> - Para atender este item, estou acessando os dados que estão sendo salvos para compor o historico de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "69f0d6f2-50e5-4dbe-a36b-58a30feda4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+\n",
      "|    name|      date|daily_avg_humidity|\n",
      "+--------+----------+------------------+\n",
      "| Colombo|2023-10-31|              95.6|\n",
      "| Colombo|2023-11-01|             94.25|\n",
      "| Colombo|2023-11-02|              90.0|\n",
      "| Colombo|2023-11-03|             93.88|\n",
      "| Colombo|2023-11-04|              70.0|\n",
      "| Colombo|2023-11-05|             85.33|\n",
      "|Curitiba|2023-10-31|              93.8|\n",
      "|Curitiba|2023-11-01|             93.25|\n",
      "|Curitiba|2023-11-02|             89.38|\n",
      "|Curitiba|2023-11-03|             93.88|\n",
      "|Curitiba|2023-11-04|              69.5|\n",
      "|Curitiba|2023-11-05|             81.33|\n",
      "|Londrina|2023-10-31|              92.2|\n",
      "|Londrina|2023-11-01|              86.5|\n",
      "|Londrina|2023-11-02|             78.75|\n",
      "|Londrina|2023-11-03|             79.13|\n",
      "|Londrina|2023-11-04|             62.38|\n",
      "|Londrina|2023-11-05|              75.0|\n",
      "| Pinhais|2023-10-31|              95.0|\n",
      "| Pinhais|2023-11-01|              94.5|\n",
      "| Pinhais|2023-11-02|             91.13|\n",
      "| Pinhais|2023-11-03|             94.13|\n",
      "| Pinhais|2023-11-04|             69.25|\n",
      "| Pinhais|2023-11-05|             81.67|\n",
      "+--------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Especifique o caminho dos dados Delta\n",
    "minio_bucket_name = \"desafiotempo\"\n",
    "minio_path = \"transient/forecast_historic\"\n",
    "format = \"delta\"\n",
    "\n",
    "table_path = f\"s3a://{minio_bucket_name}/{minio_path}\"\n",
    "\n",
    "# Carregue os dados Delta Lake\n",
    "df3 = spark.read.format(format).load(table_path)\n",
    "\n",
    "# Converter a coluna \"dt_txt\" para o nível do dia\n",
    "df3 = df3.withColumn(\"date\", to_date(df3[\"dt_txt\"]))\n",
    "\n",
    "# Calcule a média da umidade diária\n",
    "average4 = df3.groupBy(\"name\", \"date\").agg(avg(\"humidity\").alias(\"daily_avg_humidity\"))\n",
    "\n",
    "# Arredonde a coluna \"daily_avg_humidity\" para duas casas decimais\n",
    "average4 = average4.withColumn(\"daily_avg_humidity\", round(average4[\"daily_avg_humidity\"], 2))\n",
    "\n",
    "# Crie uma tabela temporária\n",
    "average4.createOrReplaceTempView(\"daily_avg_humidity_table\")\n",
    "\n",
    "spark.sql(\"\"\"SELECT name, date, daily_avg_humidity FROM daily_avg_humidity_table\n",
    "WHERE date BETWEEN '2023-10-31' AND '2023-11-06' ORDER BY name ASC, date ASC\"\"\").show(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6b991c",
   "metadata": {},
   "source": [
    "#### - Identificar as cidades com a maior e menor umidade média em um período de 30 dias e exibir os resultados em uma tabela SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a5c00",
   "metadata": {},
   "source": [
    "> - Para atender este item, estou acessando os dados que estão sendo salvos para compor o historico de tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df440dbc-bab9-49e0-b6d9-d08ae09ca0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cidades com a maior e a menor média de Umidade:\n",
      "+-------+--------+--------+--------+\n",
      "| Cidade|humidity|  Cidade|humidity|\n",
      "+-------+--------+--------+--------+\n",
      "|Colombo|   87.98|Londrina|    78.5|\n",
      "+-------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Especifique o caminho dos dados Delta\n",
    "minio_bucket_name = \"desafiotempo\"\n",
    "minio_path = \"transient/forecast_historic\"\n",
    "format = \"delta\"\n",
    "\n",
    "table_path = f\"s3a://{minio_bucket_name}/{minio_path}\"\n",
    "#lendo o delta\n",
    "df_temp2 = spark.read.format(format).load(table_path)\n",
    "df_temp2 = df_temp2.select(\"name\", \"humidity\", \"dt\")\n",
    "\n",
    "df_temp2 = df_temp2.withColumn(\"date\", from_unixtime(col(\"dt\")))\n",
    "\n",
    "# Defina a data de início e fim do intervalo de 30 dias\n",
    "data_inicio = \"2023-10-31\"\n",
    "data_fim = \"2023-11-30\"\n",
    "\n",
    "# Seleciona o intervalo de 30 dias\n",
    "df_temp2 = df_temp2.filter((col(\"date\") >= data_inicio) & (col(\"date\") <= data_fim))\n",
    "\n",
    "# Calcula a média da coluna \"humidity\"\n",
    "average_temp = df_temp2.groupBy(\"name\").agg(avg(\"humidity\").alias(\"humidity\"))\n",
    "average_temp = average_temp.withColumnRenamed(\"name\", \"Cidade\")\n",
    "\n",
    "# Arredonda a coluna \"humidity\" para duas casas decimais\n",
    "average_temp = average_temp.withColumn(\"humidity\", round(average_temp[\"humidity\"], 2))\n",
    "\n",
    "# Encontra a cidade com a maior média de Umidade\n",
    "city_with_max_temp = average_temp.orderBy(col(\"humidity\").desc()).first()\n",
    "city_with_max_temp = spark.createDataFrame([Row(**city_with_max_temp.asDict())])\n",
    "city_with_max_temp.createOrReplaceTempView(\"city_with_max_humidity\")\n",
    "\n",
    "# Encontra a cidade com a menor média de Umidade\n",
    "city_with_min_temp = average_temp.orderBy(col(\"humidity\").asc()).first()\n",
    "city_with_min_temp = spark.createDataFrame([Row(**city_with_min_temp.asDict())])\n",
    "city_with_min_temp.createOrReplaceTempView(\"city_with_min_humidity\")\n",
    "\n",
    "# Criar tabela SQL para as cidades com a maior e a menor média de Umidade:\n",
    "print(\"Cidades com a maior e a menor média de Umidade:\")\n",
    "spark.sql(\"SELECT * FROM city_with_max_humidity, city_with_min_humidity\").show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bdcb9954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sessão spark encerrada\n"
     ]
    }
   ],
   "source": [
    "# # Encerra a sessão Spark\n",
    "print(\"Sessão spark encerrada\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
